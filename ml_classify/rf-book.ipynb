{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "from timeit import timeit\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from joblib import dump, load\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiledataset import load_dataset, compile_dataset\n",
    "\n",
    "PATH = \"/home/hampus/miun/master_thesis/Datasets\"\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "# dataset: pd.DataFrame = load_dataset(PATH + \"/ORNL\", \"data_a.csv\")\n",
    "# dataset[\"remarks\"] = \"No DLC available\"\n",
    "# datasets[\"ROAD\"] = dataset.to_dict(\"records\")\n",
    "\n",
    "dataset: pd.DataFrame = load_dataset(PATH + \"/Survival\", \"data.csv\")\n",
    "dataset[\"remarks\"] = \"-\"\n",
    "datasets[\"Survival\"] = dataset.to_dict(\"records\")\n",
    "\n",
    "# dataset: pd.DataFrame = load_dataset(PATH + \"/Hisingen\", \"data.csv\")\n",
    "# dataset[\"remarks\"] = \"-\"\n",
    "# datasets[\"Hisingen\"] = dataset.to_dict(\"records\")\n",
    "\n",
    "\n",
    "df = compile_dataset(datasets)\n",
    "df.drop(columns=[\"d0\", \"d1\", \"d2\", \"d3\", \"d4\", \"d5\", \"d6\", \"d7\", \"data\", \"ID\", \"DLC\", \"t\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "dataset = None # Release memory, as it isn't used for now\n",
    "datasets = None\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns=[\"t\", \"ID\", \"data\", \"type\"], inplace=True, errors=\"ignore\")\n",
    "# df.to_csv(\"datasets.zip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"datasets.zip\")\n",
    "# display(df)\n",
    "# df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "# df.to_csv(\"datasets.zip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_tools import plot_correlation_matrix\n",
    "\n",
    "plot_correlation_matrix(df.drop(columns=[\"dataset\", \"type\", \"name\", \"ID\", \"DLC\", \"t\"], errors=\"ignore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"type\"] != \"masq\"]\n",
    "# df = df.loc[(df[\"type\"] == \"fuzz\") | (df[\"type\"] == \"none\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# index_normal = df.loc[df[\"Label\"] == 0, (df.columns != \"type\") & (df.columns != \"dataset\") & (df.columns != \"name\") & (df.columns != \"Label\")]\n",
    "scaler = StandardScaler(copy=True).fit(df.loc[df[\"Label\"] == 0, (df.columns != \"type\") & (df.columns != \"dataset\") & (df.columns != \"name\") & (df.columns != \"Label\")])\n",
    "\n",
    "df.loc[:, (df.columns != \"type\") & (df.columns != \"dataset\") & (df.columns != \"name\") & (df.columns != \"Label\")] = scaler.transform(\n",
    "    df.loc[:, (df.columns != \"type\") & (df.columns != \"dataset\") & (df.columns != \"name\") & (df.columns != \"Label\")]\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify on the sub-dataset\n",
    "X_train = df.drop(columns=\"name\")\n",
    "y_train = df[\"name\"]\n",
    "\n",
    "df = None # Release memory\n",
    "\n",
    "# Split dataset into training and test data, stratify by the type of attack\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=0, shuffle=True, stratify=y_train)\n",
    "\n",
    "# Use feature \"Label\" as classification label\n",
    "d_temp: pd.DataFrame = pd.concat([X_train, y_train], axis=\"columns\")\n",
    "X_train, y_train = d_temp.drop(columns=\"Label\"), d_temp[\"Label\"]\n",
    "d_temp: pd.DataFrame = pd.concat([X_test, y_test], axis=\"columns\")\n",
    "X_test, y_test = d_temp.drop(columns=\"Label\"), d_temp[\"Label\"]\n",
    "d_temp = None # Release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "bintr = np.bincount(y_train)\n",
    "binte = np.bincount(y_test)\n",
    "print(f\"Labels\\t\\tTraining\\tTesting\\nNormal\\t\\t{bintr[0]}\\t\\t{binte[0]}\\nAttack\\t\\t{bintr[1]}\\t\\t{binte[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = X_test.loc[(X_test[\"type\"] == \"fuzz\") | (X_test[\"type\"] == \"none\")]\n",
    "# y_test = y_test.loc[X_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = X_train.loc[X_train[\"dataset\"] == \"Survival\"].index\n",
    "test_index = X_test.loc[X_test[\"dataset\"] == \"ROAD\"].index\n",
    "\n",
    "X_train = X_train.loc[train_index]\n",
    "y_train = y_train.loc[train_index]\n",
    "\n",
    "X_test = X_test.loc[test_index]\n",
    "y_test = y_test.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=[\"type\", \"dataset\", \"name\"], inplace=True)\n",
    "X_test.drop(columns=[\"type\", \"dataset\", \"name\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=20, random_state=0, max_leaf_nodes=300, max_features=\"log2\", warm_start=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, X_train, y_train, scoring='f1', cv=10, n_jobs=-1)\n",
    "print(\"Training F1: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std()))\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "f1_scores = f1_score(y_test, pred, average='weighted')\n",
    "print(\"Testing F1:  %0.4f(+/- %0.4f)\" % (f1_scores.mean(), f1_scores.std()))\n",
    "\n",
    "kappa_scores = cohen_kappa_score(y_test, pred)\n",
    "print(\"Kappa score:  %0.4f(+/- %0.4f)\" % (kappa_scores.mean(), kappa_scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot test data predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_tools import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(y_test, pred, \"Survival, All Attacks, RF\\nrate\\n(# of instances)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot train data predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = clf.predict(X_train)\n",
    "\n",
    "plot_confusion_matrix(y_train, pred_train, \"Survival, All Attacks, RF\\nrate\\n(# of instances)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add trees trained on the FPs and FNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.set_params(n_estimators=30) # Add another 20 trees for the FN and FPs\n",
    "clf.fit(X_train.loc[y_train != pred_train], y_train.loc[y_train != pred_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot test data predictions again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = clf.predict(X_test)\n",
    "\n",
    "plot_confusion_matrix(y_test, pred_test, \"Survival, All Attacks, RF\\nrate\\n(# of instances)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from shap_tools import *\n",
    "\n",
    "\n",
    "exp = shap.TreeExplainer(clf)\n",
    "\n",
    "# # Make sure that the ingested SHAP model (a TreeEnsemble object) makes the\n",
    "# # same predictions as the original model\n",
    "# assert np.abs(exp.model.predict(X_test_sample) - clf.predict_proba(X_test_sample)).max() < 1e-4\n",
    "\n",
    "# # Make sure the SHAP values sum up to the model output (this is the local accuracy property)\n",
    "# assert np.abs(exp.expected_value + exp.shap_values(X_test_sample).sum(1) - clf.predict_proba(X_test_sample)).max() < 1e-4\n",
    "\n",
    "\n",
    "shap_false = get_explanation(exp, X_test.loc[y_test != pred])\n",
    "shap_true = get_explanation(exp, X_test.loc[y_test == pred])\n",
    "\n",
    "plot_beeswarm(shap_false)\n",
    "\n",
    "# shap_values = explainer(X_test.sample(1000, random_state=0))\n",
    "# shap_values = shap.Explanation(shap_values[:, :, 1], feature_names=X_test.columns)\n",
    "\n",
    "# shap.summary_plot(shap_values)\n",
    "\n",
    "\n",
    "# shap.waterfall_plot(shap.Explanation(values=shap_values[int(\"which_class\")][row], \n",
    "#                                          base_values=explainer.expected_value[int(which_class)], \n",
    "#                                          data=X_test.iloc[row],  # added this line\n",
    "#                                          feature_names=X_test.columns.tolist()))\n",
    "# shap.force_plot(explainer.expected_value[1], shap_values[1], features=X_test[:1], feature_names=X_test.columns)\n",
    "\n",
    "# shap.plots.scatter(shap_values[:,\"ones_w\"])\n",
    "# shap.summary_plot(shap_values[1], X_test.columns)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_beeswarm(shap_true)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cc1c01a394bbc3d069051c289161a3236732c38d026dbad3d7b5639e0cadb70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ml-classify-pJEs0r8S-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
