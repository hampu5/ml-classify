{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from timeit import timeit\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from joblib import dump, load\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiledataset import load_dataset, compile_dataset\n",
    "\n",
    "PATH = \"/home/hampus/miun/master_thesis/Datasets\"\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "# dataset: pd.DataFrame = load_dataset(PATH + \"/ORNL\", \"data_a.csv\")\n",
    "# dataset[\"remarks\"] = \"No DLC available\"\n",
    "# datasets[\"ROAD\"] = dataset.to_dict(\"records\")\n",
    "\n",
    "# dataset: pd.DataFrame = load_dataset(PATH + \"/Survival\", \"data.csv\") #, \"Malfunction_dataset_SONATA\")\n",
    "# dataset[\"remarks\"] = \"-\"\n",
    "# datasets[\"Survival\"] = dataset.to_dict(\"records\")\n",
    "\n",
    "# dataset: pd.DataFrame = load_dataset(PATH + \"/Hisingen\", \"data.csv\", \"Vehicle_F-Model_2-Fabrication_attack-Sample_1\")\n",
    "# dataset[\"remarks\"] = \"-\"\n",
    "# datasets[\"Hisingen\"] = dataset.to_dict(\"records\")\n",
    "\n",
    "\n",
    "# df = compile_dataset(datasets)\n",
    "\n",
    "# df: pd.DataFrame = load(\"dumped_datasets/survival_all.joblib\")\n",
    "# df: pd.DataFrame = load(\"dumped_datasets/road_all.joblib\")\n",
    "df: pd.DataFrame = load(\"dumped_datasets/hisingen_all.joblib\")\n",
    "\n",
    "df.drop(columns=[\"data\", \"data_dec\", \"ID\", \"DLC\", \"t\"], inplace=True, errors=\"ignore\")\n",
    "df.drop(columns=[\"d0\", \"d1\", \"d2\", \"d3\", \"d4\", \"d5\", \"d6\", \"d7\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "dataset = None # Release memory, as it isn't used for now\n",
    "datasets = None\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dt_data\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col = \"dcs_ID\"\n",
    "# v_r         = df.loc[(df[\"Label\"] == 0) & (df[\"class\"] == \"modelF\"), col]\n",
    "# v_sonata    = df.loc[(df[\"Label\"] == 0) & (df[\"class\"] == \"Sonata\"), col]\n",
    "# v_soul      = df.loc[(df[\"Label\"] == 0) & (df[\"class\"] == \"Soul\"), col]\n",
    "# v_spark     = df.loc[(df[\"Label\"] == 0) & (df[\"class\"] == \"Spark\"), col]\n",
    "\n",
    "# print(v_r.mean(), v_r.std())\n",
    "# print(v_sonata.mean(), v_sonata.std())\n",
    "# print(v_soul.mean(), v_soul.std())\n",
    "# print(v_spark.mean(), v_spark.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from load_signal_road import load_signal_road\n",
    "# df = load_signal_road()\n",
    "# df.drop(columns=[\"ID\", \"t\"], inplace=True)\n",
    "# df.fillna(-100000, inplace=True)\n",
    "# display(df)\n",
    "\n",
    "# X_train = df.drop(columns=[\"Label\"])\n",
    "# y_train = df[\"Label\"]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=0, shuffle=True, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rus = RandomUnderSampler(random_state=0)\n",
    "# X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "# X_test, y_test = rus.fit_resample(X_test, y_test)\n",
    "# bintr = np.bincount(y_train)\n",
    "# binte = np.bincount(y_test)\n",
    "# print(f\"Labels\\t\\tTraining\\tTesting\\nNormal\\t\\t{bintr[0]}\\t\\t{binte[0]}\\nAttack\\t\\t{bintr[1]}\\t\\t{binte[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# clf = RandomForestClassifier(n_estimators=20, random_state=0, max_leaf_nodes=300, max_features=\"log2\", warm_start=True)\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = cross_val_score(clf, X_train, y_train, scoring='f1', cv=10, n_jobs=-1)\n",
    "# print(\"Training F1: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# pred = clf.predict(X_test)\n",
    "\n",
    "# f1_scores = f1_score(y_test, pred, average='weighted')\n",
    "# print(\"Testing F1:  %0.4f(+/- %0.4f)\" % (f1_scores.mean(), f1_scores.std()))\n",
    "\n",
    "# kappa_scores = cohen_kappa_score(y_test, pred)\n",
    "# print(\"Kappa score:  %0.4f(+/- %0.4f)\" % (kappa_scores.mean(), kappa_scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from plot_tools import plot_confusion_matrix\n",
    "\n",
    "# pred_train = clf.predict(X_train)\n",
    "\n",
    "# plot_confusion_matrix(y_train, pred_train, \"Survival, All Attacks, RF\\nrate\\n(# of instances)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "# exp = shap.TreeExplainer(clf, data=X_train, model_output=\"probability\")\n",
    "\n",
    "# from shap_tools import *\n",
    "# shap_all = get_explanation(exp, X_train, 600)\n",
    "\n",
    "# plot_beeswarm(shap_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Above ^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"type\"] != \"masq\"]\n",
    "# df = df.loc[(df[\"type\"] == \"fuzz\") | (df[\"type\"] == \"none\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from plot_tools import plot_correlation_matrix\n",
    "\n",
    "# plot_correlation_matrix(df.drop(columns=[\"dataset\", \"type\", \"name\", \"class\" \"ID\", \"DLC\", \"t\"], errors=\"ignore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# feature_columns= list(set(df.columns.to_list()).difference([\"name\", \"class\", \"dataset\", \"type\", \"Label\"]))\n",
    "\n",
    "# for col in feature_columns:\n",
    "#     scaler = StandardScaler().fit(df.loc[df[\"Label\"] == 0, df.columns == col])\n",
    "#     df.loc[:, df.columns == col] = scaler.transform(df.loc[:, df.columns == col])\n",
    "\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify on the sub-dataset\n",
    "X_train = df.drop(columns=\"Label\")\n",
    "y_train = df[\"Label\"]\n",
    "\n",
    "df = None # Release memory\n",
    "\n",
    "# Split dataset into training and test data, stratify by the type of attack\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=0, shuffle=True, stratify=X_train[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = df.loc[df[\"type\"] != \"normal\"]\n",
    "# y_train = X_train[\"Label\"]\n",
    "# X_train = X_train.drop(columns=\"Label\")\n",
    "\n",
    "# X_test = df.loc[df[\"type\"] == \"normal\"]\n",
    "# y_test = X_test[\"Label\"]\n",
    "# X_test = X_test.drop(columns=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rus = RandomUnderSampler(random_state=0)\n",
    "# X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "# X_test, y_test = rus.fit_resample(X_test, y_test)\n",
    "# bintr = np.bincount(y_train)\n",
    "# binte = np.bincount(y_test)\n",
    "# print(f\"Labels\\t\\tTraining\\tTesting\\nNormal\\t\\t{bintr[0]}\\t\\t{binte[0]}\\nAttack\\t\\t{bintr[1]}\\t\\t{binte[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_index = X_train.loc[X_train[\"dataset\"] == \"Survival\"].index\n",
    "# test_index = X_test.loc[X_test[\"dataset\"] == \"ROAD\"].index\n",
    "# train_index = X_train.loc[(X_train[\"dataset\"] == \"Survival\") & (X_train[\"name\"] == \"Fuzzy_dataset_SONATA\")].index\n",
    "# test_index = X_test.loc[(X_test[\"dataset\"] == \"ROAD\") & (X_test[\"name\"] == \"fuzzing_attack_1\")].index\n",
    "\n",
    "# X_train = X_train.loc[train_index]\n",
    "# y_train = y_train.loc[train_index]\n",
    "\n",
    "# X_test = X_test.loc[test_index]\n",
    "# y_test = y_test.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_train = X_train[\"name\"]\n",
    "# name_test = X_test[\"name\"]\n",
    "X_train.drop(columns=[\"type\", \"dataset\", \"name\", \"class\"], inplace=True)\n",
    "X_test.drop(columns=[\"type\", \"dataset\", \"name\", \"class\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pp_tools import scale_dataset\n",
    "\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()\n",
    "\n",
    "X_combined = pd.concat([X_train, X_test])\n",
    "\n",
    "scale_dataset(X_combined)\n",
    "\n",
    "test_len = len(X_test)\n",
    "X_test = X_combined.iloc[-test_len:]\n",
    "X_train = X_combined.iloc[:-test_len]\n",
    "\n",
    "X_combined = None # Release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=20, random_state=0, max_leaf_nodes=300, max_features=\"log2\", warm_start=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_train = clf.predict(X_train)\n",
    "# clf.set_params(n_estimators=(20+13)) # Add another tree for the FN and FPs\n",
    "# clf.fit(X_train.loc[y_train != pred_train], y_train.loc[y_train != pred_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, f1_score, accuracy_score, matthews_corrcoef\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "acc_scores_train = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "print(\"Training Acc: %0.4f (+/- %0.8f)\" % (acc_scores_train.mean(), acc_scores_train.std()))\n",
    "\n",
    "f1_scores_train = cross_val_score(clf, X_train, y_train, scoring='f1', cv=10, n_jobs=-1)\n",
    "print(\"Training F1: %0.4f (+/- %0.4f)\" % (f1_scores_train.mean(), f1_scores_train.std()))\n",
    "\n",
    "acc_scores = accuracy_score(y_test, pred)\n",
    "# acc_scores = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "print(\"Testing Accuracy:  %0.4f (+/- %0.4f)\" % (acc_scores.mean(), acc_scores.std()))\n",
    "\n",
    "f1_scores = f1_score(y_test, pred, average='weighted')\n",
    "# f1_scores = cross_val_score(clf, X_train, y_train, scoring='f1', cv=10, n_jobs=-1)\n",
    "print(\"Testing F1:  %0.4f (+/- %0.4f)\" % (f1_scores.mean(), f1_scores.std()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# acc_scores = accuracy_score(y_test, pred)\n",
    "# print(\"Testing Accuracy:  %0.4f (+/- %0.4f)\" % (acc_scores.mean(), acc_scores.std()))\n",
    "\n",
    "# # f1_scores_test = f1_score(y_test, pred, average='weighted')\n",
    "# f1_scores_test = cross_val_score(clf, X_test, y_test, scoring='f1', cv=10, n_jobs=-1)\n",
    "# print(\"Testing F1:  %0.4f (+/- %0.4f)\" % (f1_scores_test.mean(), f1_scores_test.std()))\n",
    "\n",
    "\n",
    "# kappa_scores = cohen_kappa_score(y_test, pred)\n",
    "# # kappa_scores = cross_val_score(clf, X_test, y_test, scoring='kappa', cv=10, n_jobs=-1)\n",
    "# print(\"Kappa score:  %0.4f\" % (kappa_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot train then test data predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_tools import plot_confusion_matrix\n",
    "\n",
    "pred_train = clf.predict(X_train)\n",
    "\n",
    "plot_confusion_matrix(y_train, pred_train, \"RF, \\\"Survival\\\", all attacks, training data\\n(# of instances)\", cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, pred, \"RF, \\\"Survival\\\", all attacks, testing data\\n(# of instances)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, pred, normalize=\"true\"))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# # RocCurveDisplay.from_predictions(y_test, pred, name=\"RF\")\n",
    "# RocCurveDisplay.from_estimator(clf, X_test, y_test, name=\"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "exp = shap.TreeExplainer(clf, data=X_train, model_output=\"probability\")\n",
    "# exp = shap.KernelExplainer(clf.predict_proba, data=X_train.sample(100, random_state=1))\n",
    "print(exp.expected_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shap_tools import *\n",
    "\n",
    "\n",
    "# Make sure that the ingested SHAP model (a TreeEnsemble object) makes the\n",
    "# same predictions as the original model\n",
    "# assert np.abs(exp.model.predict(X_train) - clf.predict_proba(X_train)).max() < 1e-4\n",
    "\n",
    "X_train_exp = X_train.sample(min(len(X_train), 600), random_state=0)\n",
    "y_train_exp = y_train[X_train_exp.index]\n",
    "\n",
    "shap_all = get_explanation(exp, X_train_exp)\n",
    "\n",
    "# dump(shap_all, \"rf_survival_all_shap\")\n",
    "\n",
    "# # Make sure the SHAP values sum up to the model output (this is the local accuracy property)\n",
    "# assert np.abs((shap_all.base_values + shap_all.values).sum(1) - clf.predict_proba(X_train)).max() < 1e-4\n",
    "\n",
    "# print(len(X_train.loc[y_train != pred]))\n",
    "# print(len(X_train.loc[y_train == pred]))\n",
    "# shap_false = get_explanation(exp, X_train.loc[y_train != pred])\n",
    "# shap_FP = get_explanation(exp, X_train.loc[(y_train != pred) & (pred == 1)])\n",
    "# shap_FN = get_explanation(exp, X_train.loc[(y_train != pred) & (pred == 0)])\n",
    "# shap_true = get_explanation(exp, X_train.loc[y_train == pred])\n",
    "# shap_TP = get_explanation(exp, X_train.loc[(y_train == pred) & (pred == 1)])\n",
    "# shap_TN = get_explanation(exp, X_train.loc[(y_train == pred) & (pred == 0)])\n",
    "\n",
    "# shap_TP_fuzz = get_explanation(exp, X_test.loc[(y_train == pred) & (pred == 1) & (name_test == \"Fuzzy_dataset_SONATA\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(shap_all[:1,\"dt_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sample = X_test.sample(10, random_state=7)\n",
    "y_test_sample = y_test.loc[X_test_sample.index]\n",
    "pred_sample = pred[X_test_sample.index]\n",
    "print(X_test_sample)\n",
    "print(y_test_sample)\n",
    "print(pred_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_beeswarm(shap_all)\n",
    "def plot_beeswarm2(exp_obj):\n",
    "    vis = shap.plots.beeswarm(exp_obj, show=False, max_display=20 , color=plt.get_cmap(\"plasma\"), order=exp_obj.feature_names)\n",
    "    plt.gcf().axes[-1].set_aspect(100)\n",
    "    plt.gcf().axes[-1].set_box_aspect(100)\n",
    "    return vis\n",
    "\n",
    "shap_all.feature_names = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "plot_beeswarm2(shap_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "def plot_test(df_exp: pd.DataFrame, shap_all: shap.Explanation, feature, trim=None, y_squish=10, scale=1, colorbar=False, y_size=1):\n",
    "    df_exp[\"Label\"].replace({0: \"normal\", 1: \"attack\"}, inplace=True)\n",
    "    shap_exp = shap_all.values[:,df_exp.columns.get_loc(feature)]\n",
    "\n",
    "    mask = None\n",
    "    if trim == None:\n",
    "        mask = (np.abs(stats.zscore(df_exp[feature])) < 3)\n",
    "    else:\n",
    "        mask = (df_exp[feature] > trim[0]) & (df_exp[feature] < trim[1])\n",
    "\n",
    "    attack_outliers = shap_exp[~mask & (df_exp[\"Label\"] == \"attack\")]\n",
    "    normal_outliers = shap_exp[~mask & (df_exp[\"Label\"] == \"normal\")]\n",
    "\n",
    "    shap_exp = shap_exp[mask]\n",
    "    df_exp = df_exp[mask]\n",
    "\n",
    "    # plt.figure(figsize=(20, 2), dpi=100)\n",
    "    # plt.figure(80, y_squish)\n",
    "    # fig = plt.figure() \n",
    "    # ax = fig.add_axes([0, 0, 80, y_squish])\n",
    "    fig, ax = plt.subplots(figsize=(80, y_squish))\n",
    "    \n",
    "    cmap_name = \"icefire\"\n",
    "    violin_color = \"lightgray\"\n",
    "\n",
    "    scaler = max(abs(shap_exp.min()), abs(shap_exp.max()))\n",
    "    shap_hues = shap_exp / scaler\n",
    "    shap_hues = (shap_hues + 1) * shap_all.base_values[0]\n",
    "\n",
    "    if attack_outliers.size != 0:\n",
    "        attack_outliers /= max(abs(attack_outliers.min()), abs(attack_outliers.max()))\n",
    "        attack_outliers = (attack_outliers + 1) * shap_all.base_values[0]\n",
    "\n",
    "    if normal_outliers.size != 0:\n",
    "        normal_outliers /= max(abs(normal_outliers.min()), abs(normal_outliers.max()))\n",
    "        normal_outliers = (normal_outliers + 1) * shap_all.base_values[0]\n",
    "    \n",
    "    cmap = sns.color_palette(cmap_name, as_cmap=True)\n",
    "    norm = plt.Normalize(vmin=0, vmax=1)\n",
    "    palette = {h: cmap(h) for h in shap_hues}\n",
    "\n",
    "    values = df_exp[feature]\n",
    "    feature_min = values.min()\n",
    "    feature_max = values.max()\n",
    "    values = (values - values.min()) / (values.max() - values.min())\n",
    "    label = df_exp[\"Label\"]\n",
    "\n",
    "    sns.swarmplot(x=values, y=label, order=[\"normal\", \"attack\"],\n",
    "        hue=shap_hues, orient=\"h\", palette=palette,\n",
    "        size=5)\n",
    "    \n",
    "    # Change offset on dots for normal (0)\n",
    "    offsets = ax.collections[0].get_offsets()\n",
    "    offsets = [[elem[0], -abs(elem[1] - 0) - 0.03] for elem in offsets]\n",
    "    ax.collections[0].set_offsets(offsets)\n",
    "\n",
    "    # Change offset on dots for attack (1)\n",
    "    offsets = ax.collections[1].get_offsets()\n",
    "    offsets = [[elem[0], abs(elem[1] - 1) + 0.07] for elem in offsets]\n",
    "    ax.collections[1].set_offsets(offsets)\n",
    "\n",
    "    fig.set_size_inches(10, y_size)\n",
    "\n",
    "    sns.violinplot(x=values, y=[0]*label.size, hue=label, split=True, hue_order=[\"normal\", \"attack\"],\n",
    "        orient=\"h\",  showfliers=False, scale=\"count\", bw=0.2, gridsize=1000, linewidth=0, color=violin_color,\n",
    "        cut=0, inner=None)\n",
    "    \n",
    "    for violin in ax.findobj(matplotlib.collections.PolyCollection):\n",
    "        violin.set_facecolor(\"lightgray\")\n",
    "    \n",
    "    ax.legend_.remove()\n",
    "\n",
    "    if colorbar:\n",
    "        cbar = plt.colorbar(plt.cm.ScalarMappable(cmap=cmap_name), label=\"contribution\", location=\"bottom\", shrink=0.2, anchor=(0.95, -1))\n",
    "        cbar.set_ticks([0, 0.5, 1])\n",
    "        cbar.set_ticklabels([\"towards\\nnormal\", \"none\", \"towards\\nattack\"])\n",
    "\n",
    "    f_dict = {\"dcs\": \"A\", \"dcs_ID\": \"B\", \"dt\": \"C\", \"dt_ID\": \"D\", \"dt_data\": \"E\"}\n",
    "    feature = f_dict[feature]\n",
    "    # ax.set_title(f\"How the RF-model classifies data - feature: {feature}\")\n",
    "    # ax.set_title(feature)\n",
    "    # feature = feature + \" (ms)\" if feature[0:2] == \"dt\" else feature\n",
    "\n",
    "    # ax.set_ylabel(\"type of data\")\n",
    "    ax.set_ylabel(feature, rotation=\"horizontal\", x=-1, y=0.4)\n",
    "    # ax.set_xlabel(f\"value of {feature}\")\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "    ax.set_xticks(ticks=np.linspace(0, 1, 10), labels=map(lambda x: format(x*scale, '.2f'), np.linspace(feature_min, feature_max, 10)))\n",
    "    ax.set_yticks(ticks=[-0.25, 0.25], labels=[\"normal\", \"attack\"])\n",
    "    ax.axhline(y=0, color=\"black\", linewidth=0.5)\n",
    "    \n",
    "    s_last = ax.get_xticks()[-2]\n",
    "    last = ax.get_xticks()[-1]\n",
    "    \n",
    "    if normal_outliers.size != 0:\n",
    "        ax.arrow(last + (last-s_last) * 0.5, -0.25, last-s_last, 0, facecolor=cmap(normal_outliers.mean()), edgecolor=violin_color,\n",
    "            width=0.07, head_length=(last-s_last)*0.7, head_width=0.2,\n",
    "            length_includes_head=True)\n",
    "    if attack_outliers.size != 0:\n",
    "        ax.arrow(last + (last-s_last) * 0.5, 0.25, last-s_last, 0, facecolor=cmap(attack_outliers.mean()), edgecolor=violin_color,\n",
    "            width=0.07, head_length=(last-s_last)*0.7, head_width=0.2,\n",
    "            length_includes_head=True)\n",
    "    \n",
    "    ax.margins(x=0.02)\n",
    "\n",
    "    # return ax\n",
    "    plt.show()\n",
    "\n",
    "plot_test(pd.concat([X_train_exp, y_train_exp], axis=1), shap_all, \"dcs\", trim=(-1, 1), y_squish=25)\n",
    "plot_test(pd.concat([X_train_exp, y_train_exp], axis=1), shap_all, \"dcs_ID\", trim=(-1, 1), y_squish=15)\n",
    "plot_test(pd.concat([X_train_exp, y_train_exp], axis=1), shap_all, \"dt\", trim=(0, 0.0003), scale=1000, y_squish=20)\n",
    "plot_test(pd.concat([X_train_exp, y_train_exp], axis=1), shap_all, \"dt_ID\", trim=(0, 0.02), scale=1000)\n",
    "plot_test(pd.concat([X_train_exp, y_train_exp], axis=1), shap_all, \"dt_data\", trim=(0, 0.02), scale=1000, colorbar=True, y_size=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_exp(pd.concat([X_train_exp, y_train_exp], axis=1), shap_all, \"dt_ID\", trim=(0, 0.02), scale=1000)\n",
    "plot_exp(pd.concat([X_train_exp, y_train_exp], axis=1), shap_all, \"dt_data\", trim=(0, 0.02), scale=1000)\n",
    "plot_exp(pd.concat([X_train_exp, y_train_exp], axis=1), shap_all, \"dt\", trim=(0, 0.0003), scale=1000, y_squish=20)\n",
    "plot_exp(pd.concat([X_train_exp, y_train_exp], axis=1), shap_all, \"dcs\", trim=(-1, 0.7), y_squish=25)\n",
    "plot_exp(pd.concat([X_train_exp, y_train_exp], axis=1), shap_all, \"dcs_ID\", trim=(0, 0.8), y_squish=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap_TP_fuzz = get_explanation(exp, X_train.loc[(y_train == pred_train) & (pred_train == 1) & (name_train == \"Fuzzy_dataset_SONATA\")])\n",
    "# plot_waterfall(shap_TP_fuzz, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_force(shap_all[65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dependence(shap_all, \"dt_ID\", \"dcs_ID\", xmax=\"percentile(99)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(shap_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(pd.Series(shap_all.data[4], index=shap_all.feature_names))\n",
    "plot_waterfall(shap_all, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = shap.decision_plot(shap_all.base_values[1],\n",
    "    shap_all.values,\n",
    "    pd.DataFrame(shap_all.data, columns=shap_all.feature_names),\n",
    "    feature_order='hclust',\n",
    "    return_objects=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argpartition(shap_all.values[:, pd.DataFrame(shap_all.data, columns=shap_all.feature_names).columns.get_loc('dt')], -2)[-1]\n",
    "shap.decision_plot(shap_all.base_values[1],\n",
    "    shap_all.values[idx],\n",
    "    pd.DataFrame(shap_all.data, columns=shap_all.feature_names),\n",
    "    feature_order=r.feature_idx,\n",
    "    xlim=r.xlim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_force(shap_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature1 = \"dcs_ID\"\n",
    "# idx = shap_all.feature_names.index(feature1)\n",
    "# shap_sums = np.sum(shap_all.values, axis=1)\n",
    "# rule1 = X_train.sample(600, random_state=0)[(shap_sums > 0) & (shap_all.values[:,idx] > 0)][feature1].min()\n",
    "# rule1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature2 = \"dt_ID\"\n",
    "# idx = shap_all.feature_names.index(feature1)\n",
    "# shap_sums = np.sum(shap_all.values, axis=1)\n",
    "# rule2 = X_train.sample(600, random_state=0)[(shap_sums > 0) & (shap_all.values[:,idx] < 0)][feature1].max()\n",
    "# rule2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rule_predictor(X: pd.DataFrame, y: pd.DataFrame):\n",
    "#     mask = (X[feature1] >= rule1) | (X[feature2] >= rule2)\n",
    "#     pred_out = y.copy() * 0\n",
    "#     pred_out.loc[mask] = 1\n",
    "#     # for i, row in X.iterrows():\n",
    "#     return pred_out\n",
    "\n",
    "# preddy = rule_predictor(X_test, y_test)\n",
    "\n",
    "# plot_confusion_matrix(y_test, preddy, \"Rule classification\")\n",
    "\n",
    "# zeros = np.bincount(y_test.loc[mask])[0]\n",
    "# ones = np.bincount(y_test.loc[mask])[1]\n",
    "# # print(zeros)\n",
    "# print(1 - zeros/ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# pred_train = clf.predict(X_train)\n",
    "# cm = confusion_matrix(y_train, pred_train)\n",
    "\n",
    "# result = [[cm[0][1]], [cm[1][0]]]\n",
    "\n",
    "# for i in range(1, 31):\n",
    "#     clf.set_params(n_estimators=(20+i)) # Add another tree for the FN and FPs\n",
    "#     clf.fit(X_train.loc[y_train != pred_train], y_train.loc[y_train != pred_train]) # 1 for FP, 0 for FN\n",
    "#     pred_train = clf.predict(X_train)\n",
    "#     cm = confusion_matrix(y_train, pred_train)\n",
    "#     result[0].append(cm[0][1])\n",
    "#     result[1].append(cm[1][0])\n",
    "\n",
    "# print(result[0])\n",
    "# print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(result[0], label=\"FP\")\n",
    "# plt.plot(result[1], label=\"FN\")\n",
    "# plt.title(\"Change in FP and FN predictions\\nwith more trees trained on FP and FN instances\")\n",
    "# plt.xlabel(\"Number of additional trees (originally 20)\")\n",
    "# plt.ylabel(\"Number of instances\")\n",
    "# plt.legend()\n",
    "# plt.show"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cc1c01a394bbc3d069051c289161a3236732c38d026dbad3d7b5639e0cadb70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ml-classify-pJEs0r8S-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
