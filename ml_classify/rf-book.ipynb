{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "# matplotlib.use(\"pgf\")\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from joblib import dump, load\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import shap\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiledataset import load_dataset, compile_dataset\n",
    "\n",
    "PATH = \"/home/hampus/miun/master_thesis/Datasets\"\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "# dataset: pd.DataFrame = load_dataset(PATH + \"/ORNL\", \"data_a.csv\")\n",
    "# dataset[\"remarks\"] = \"No DLC available\"\n",
    "# datasets[\"ROAD\"] = dataset.to_dict(\"records\")\n",
    "\n",
    "dataset: pd.DataFrame = load_dataset(PATH + \"/Survival\", \"data.csv\")\n",
    "dataset[\"remarks\"] = \"-\"\n",
    "datasets[\"Survival\"] = dataset.to_dict(\"records\")\n",
    "\n",
    "# dataset: pd.DataFrame = load_dataset(PATH + \"/Hisingen\", \"data.csv\")\n",
    "# dataset[\"remarks\"] = \"-\"\n",
    "# datasets[\"Hisingen\"] = dataset.to_dict(\"records\")\n",
    "\n",
    "\n",
    "df = compile_dataset(datasets)\n",
    "\n",
    "dataset = None # Release memory, as it isn't used for now\n",
    "datasets = None\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"d0\", \"d1\", \"d2\", \"d3\", \"d4\", \"d5\", \"d6\", \"d7\", \"ID\", \"DLC\", \"t\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "df.loc[:, (df.columns != \"type\") & (df.columns != \"dataset\")] = MinMaxScaler(copy=True).fit_transform(df.loc[:, (df.columns != \"type\") & (df.columns != \"dataset\")])\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(columns=\"type\")\n",
    "y_train = df[\"type\"]\n",
    "\n",
    "df = None # Release memory\n",
    "\n",
    "# Split dataset into training and test data, stratify by the type of attack\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=0, shuffle=True, stratify=y_train)\n",
    "\n",
    "# Use feature \"Label\" as classification label\n",
    "d_temp: pd.DataFrame = pd.concat([X_train, y_train], axis=\"columns\")\n",
    "X_train, y_train = d_temp.drop(columns=\"Label\"), d_temp[\"Label\"]\n",
    "d_temp: pd.DataFrame = pd.concat([X_test, y_test], axis=\"columns\")\n",
    "X_test, y_test = d_temp.drop(columns=\"Label\"), d_temp[\"Label\"]\n",
    "d_temp = None # Release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "bintr = np.bincount(y_train)\n",
    "binte = np.bincount(y_test)\n",
    "print(f\"Labels\\t\\tTraining\\tTesting\\nNormal\\t\\t{bintr[0]}\\t\\t{binte[0]}\\nAttack\\t\\t{bintr[1]}\\t\\t{binte[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=[\"type\", \"dataset\"], inplace=True)\n",
    "X_test.drop(columns=[\"type\", \"dataset\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=20, random_state=0, max_leaf_nodes=300).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, X_train, y_train, scoring='f1', cv=10, n_jobs=-1)\n",
    "print(\"Training F1: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std()))\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "f1_scores = f1_score(y_test, pred, average='weighted')\n",
    "print(\"Testing F1:  %0.4f(+/- %0.4f)\" % (f1_scores.mean(), f1_scores.std()))\n",
    "\n",
    "kappa_scores = cohen_kappa_score(y_test, pred)\n",
    "print(\"Kappa score:  %0.4f(+/- %0.4f)\" % (kappa_scores.mean(), kappa_scores.std()))\n",
    "\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "annots = pd.DataFrame(confusion_matrix(y_test, pred)).applymap(str)\n",
    "sns.heatmap(cm, annot=annots, fmt=\"s\", square=True)\n",
    "plt.show()\n",
    "\n",
    "# Not under-sampled:    FN=387, FP=286\n",
    "# Under-sampled:        FN=119, FP=1149\n",
    "# Under-sampled (type): FN=176, FP=1363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cc1c01a394bbc3d069051c289161a3236732c38d026dbad3d7b5639e0cadb70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ml-classify-pJEs0r8S-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
